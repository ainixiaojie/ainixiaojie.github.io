[{"title":"Mathematical_Foundations_of_Reinforcement_Learning","url":"/2023/11/04/Mathematical-Foundations-of-Reinforcement-Learning/","tags":["Mathematical_Foundations_of_Reinforcement_Learning"]},{"title":"reinforcement_learning_Bellman_equation","url":"/2023/11/03/reinforcement-learning-Bellman-equation/","content":"# 贝尔曼公式 Bellman_equation\n\n用来计算。当前的全局策略下，状态为s。可以获得的return（一系列动作至达到目标，或者无限延续的连续动作序列，所获的奖励值）\n\n这个return。是一个期望值，是所有的可能的动作序列的期望奖励。\n\n这个return值可以认为是当前策略下，状态s的值为return的值  状态值  state value\n\n\n\n举例：\n    五个状态，初始时，五个状态值\n    [0,0,0,0,0]\n    动作只有一个。所以每个状态都只能执行该动作。\n    奖励值如下：\n    第五个状态就是目的地。所以第四个状态执行动作，获得奖励值为1。其余所有状态，执行动作获得的奖励值都为0\n    [0,0,0,1,0]\n\n    经过第一次动作迭代,1-2-3-4-5.结束\n    guma=0.1\n\n    s_value=reward+guma*s_next_value\n\n    执行第一个动作\n    s_1_value=0_reward+guma*s_2_value\n    s_1_value=0+0.1*0=0\n\n    执行第二个动作\n    s_2_value=1_reward+guma*s_3_value\n    s_2_value=0+0.1*0=0\n\n    执行第三个动作\n    s_3_value=0_reward+guma*s_4_value\n    s_3_value=0+0.1*0=0\n\n    执行第四个动作\n    s_4_value=0_reward+guma*s_5_value\n    s_4_value=1+0.1*0=1\n\n\n    -----第一次到达目的地，完成了一回合的学习\n    状态值：\n    [0,0,0,**0**,0]---->[0,0,0,**1**,0]\n    ---- 第二回合来了\n    经过动作迭代,1-2-3-4-5.结束\n    guma=0.1\n\n    s_value=reward+guma*s_next_value\n\n    执行第一个动作\n    s_1_value=0_reward+guma*s_2_value\n    s_1_value=0+0.1*0=0\n\n    执行第二个动作\n    s_2_value=1_reward+guma*s_3_value\n    s_2_value=0+0.1*0=0\n\n    执行第三个动作\n    s_3_value=0_reward+guma*s_4_value\n    s_3_value=0+0.1*1=0.1\n\n    执行第四个动作\n    s_4_value=0_reward+guma*s_5_value\n    s_4_value=1+0.1*0=1\n\n    -----第二次到达目的地，完成了二回合的学习\n    状态值：\n    [0,0,**0**,1,0]---->[0,0,**0.1**,1,0]\n\n\n    由此可推:\n    第三次\n    [0,**0**，0.1,1,0]---->[0,**0.01**,0.1,1,0]\n    第四次\n    [**0**，0。01,0.1,1,0]---->[**0.001**,001,0.1,1,0]\n\n\n\n\nAction value 动作值\n状态值:\n这里设定，每个状态只有一个动作。所以动作的概率值为1.没得挑\ns_value=reward+guma*s_next_value\n实际上：\ns_value={a_probablityity  * [a_reward+guma*s_next_value]}的期望\n\n状态值={动作的概率*（动作reward+动作后所到达的状态的状态值）}的期望。\n动作值=（动作reward+动作后所到达的状态的状态值）\na_value=a_reward+guma*s_next_value\n\n\n# 贝尔曼最优公式 Bellman_optimality_equation\n","tags":["reinforcement_learning"]},{"title":"reinforcement_learning_Actor","url":"/2023/11/02/reinforcement-learning-Actor/","content":"# Actor 行动者/演员\n\n这是一种agent（智能体）\n\n输入：状态\n\n使用**神经网络**\n\n输入：每个动作的概率值\n","categories":["reinforcement_learning"]},{"title":"reinforcement_learning 概论","url":"/2023/11/02/reinforcement-learning/","content":"# Reinforcement Learning    强化学习\n\n智能体与环境的交互。\n\n智能体  根据环境的状态  提供动作给环境\n环境    执行了动作  改变了环境的状态    反馈奖励给智能体（状态信息也反馈给智能体）\n\n智能体在根据自己提供的动作与环境反馈的奖励，不断学习。\n环境根据智能体提供的动作不断变化状态，反馈奖励。\n\n## 智能体\n\n智能体就三个行为,选择动作,预测动作,学习\n\n### 1，选择动作 Sample\n输入：环境的状态\n输出：智能体选择的动作\n\n常见的。e-greedy策略\n假设e=0.3\n生成一个随机数,范围[0，1]。\n若是随机数小于e。则从所有的动作中随机选择一个动作\n否则，使用预测Predict。他会选择当前最优的动作。\n\n    这个通常是在学习的时候使用这个选择动作 Sample。\n    若是学习完成了，则就直接使用Predict了。\n\n一定概率的随机选择，可以走出更多不同的动作。放置陷入局部最优。\n\n    三个动作，[1，2，3]对应奖励 [1，2，3]\n    但是默认，一开始的时候。我们认为所有动作的奖励值都为0。[0，0，0]\n    奖励值一样，第一次执行动作1，得到奖励1。更新奖励值[1，0，0]\n    若是没有随机。那么以后都会选择动作1。无法学习到真正最好的动作。\n\n### 2，预测动作 Predict\n输入：环境的状态\n\n输出：智能体当前策略的最优的动作/或者价值最高的动作\n\n### 3，学习 Learn\n\n输入：环境的状态，奖励，（可选择的：智能体选择执行的动作）\n\n输出：更新智能体的策略（函数，模型，这些都是一样的东西，就是智能体选择动作的方法）。\n\n## 环境\n\n设定与行为\n\n    设定：\n        状态的设定\n        动作的设定\n        奖励的设定\n    行为：\n        重置环境\n        执行动作（默认执行）\n        计算奖励（后台执行，执行动作后自动进行）\n\n### 状态的设定 States/Observation\n环境的状态就是一条信息，一条数据。可以唯一表示一个状态。\n\n最常见的就是用一条向量代表一个状态。\n\n用一个图片代表一个状态。\n\n所有可能的状态组合在一起，称之为观测空间**Observation_Space**。\n这只是一个概念。重点还是放在状态的表示。\n\n状态的表示有离散型和连续性。\n假设数值范围[1，10]。\n\n    离散型状态：\n        离散值设置1。10个状态。每个数字代表一个状态。1，2，3，4，5，6，7，8，9，10\n        离散值设置2。5个状态。1，3，5，7，9。\n        ...\n    连续型状态：\n        无数个状态：0.1，4.675，7.334等等\n\n假设状态：\n\n    [时间，水库水位下降百分比]：一个状态由这两个特征表示\n    时间：只可取小时为单位，则离散值1-24\n    水库水位下降百分比：若不限制小数个数。随意取值，连续。无数个\n    该观测空间也是连续的。\n\n\n<mark> 这里还有个设定，环境信息的设定</mark>\n\n    环境的附加信息。\n    常常是一些不会变化的信息。\n    譬如一个打boss游戏。\n        环境状态就是boss的状态\n        动作就是角色的技能\n\n        附加的信息：\n            boss的名字。\n            角色的信息。\n            ...\n\n### 动作的设定 Actions\n\n动作的设定与状态的设定一样。离散型与连续型。\n\n    假设动作[上，下，左，右]------离散型\n    也可假设[方向，距离（该值连续）]-----连续性\n\n所有的动作集合一起称之为动作空间 **Action_Space**\n\n### 奖励的设定 Reward\n\n仁者见仁，智者见智\n打怪游戏：boss下降的血量值\n迷宫游戏：只有到达终点才给奖励值。只要没到终点，奖励值-1。\n。。。\n\n### 执行动作 Step\n\n仁者见仁，智者见智\n\n这个行为总的来说：执行动作，计算奖励，给与反馈。这三个小步骤\n\n    1，执行动作，重点就是这个动作的逻辑\n    如何执行动作，打怪游戏，动作是释放一技能。技能伤害是100，护甲50，是否暴击（暴击两倍伤害）：是。boss血量=boss血量-（100-50）*2。\n\n    2，计算奖励：（100-50）*2。当然这是假设的奖励机制。也可以造成伤害就奖励100.类似这种。仁者见仁，智者见智\n    3，给与反馈，要有返回值：状态，奖励，是否终止，等等\n\n动作的执行逻辑部分，也是有自己设定\n\n### 计算奖励\n\n仁者见仁，智者见智","tags":["reinforcement_learning"],"categories":["reinforcement_learning"]},{"title":"pytorch","url":"/2023/08/22/pytorch/","content":"# 机器学习流程\n\n## 第一步，准备好数据集\n\n准备数据集。拿到了大量的数据。\n当然如果是强化学习，我还没坐过，还不知道，需不需要数据集。\n\n## 第二部，数据处理\n\n其实第一步，也应该在这一步里面吧\n\n要对数据进行处理。\n。\n### 数据特征的选取\n数据集的数据，不一定都是我们需要的。所以要进行一定的筛选\n数据集的一列，这个列名就是特征的名字。有的特征没有意义，可以不要。\n\n### 数据集打乱\n就是打乱数据的顺序，有的数据集会按照某一列（标签列，称之为标签，就是结果，答案，我们要用模型去预测结果，使得预测的结果尽可能符合实际的结果）\n如果存在一定顺序的话，可能会影响训练的效果，譬如模型会倾向数据按顺序的最后的一个结果。\n当然了，如果这个数据集的按照时间排列，模型就是需要抓取这种时序性，譬如RNN，循环神经网络（会根据以前的信息来预测下一的输出结果）。这种应该就不用\n\n### 一条输入数据的size的调整\n\n一张图片数据 3*width*height\n（\n举例：3*1920*1080：宽是1920，高是1080，对应的每个位置是一个像素点。而每个像素点，由RGB 3个数值组成 \n所以有三个二维矩阵，第一个矩阵对应所有像素点R的数值，第二矩阵对应所有像素点G的数值，第三个B的。一个数值就是一个浮点数（0-255）\n每一个矩阵在机器学习中，称为一个通道\n\n所以一张图片，在机器学习中：就是一个三通道，宽是1920，高是1080的数据而已\nsize:[3,1920,1080]\n）\n但是呀，大量的数据，不可能一个数据一个数据的去训练，所以有batch_size这个超参数，就是一次多少条数据\n（\n再解释一个名词,超参数，为什么是超参数，因为不是实际的参数，但又比参数要重要，所以称之为超参数，\n解释一下参数，参数就是模型的参数，在神经网路中，模型，就是由好多个好多个二维矩阵组成。二维矩阵中的每一个数值，就是 一个参数\n所以超参数，会直接影响模型。超级，名副其实。\n）\npytorch，也默认不是一条数据 一条数据的训练\n一次进入模型很多条数据\nsize进化 ：[batch_size,channel,width,height]，[10,3,1920,1080]\n\n### 数据集Datasets\n\nDatasets的作用是将原始的数据转换为模型能够处理的格式。\n它提供了一些基本的方法，比如__len__用于获取数据集的大小，\n__getitem__用于按索引获取单个数据样本。\n\n可能某一列的数据，不符合，譬如一个字符串转换成列表（可能），有的浮点数可能需要转换成整数（少，举例子而已）\n\n大部分浮点数，因为pytorch，和计算机发达。默认64位浮点数，甚至有的双精度。128位的浮点数。\n但是大量的数据集，精度高，会加大计算量，very 大。所以，需要手动的去给数据设置数据格式，numpy.float32，torch.float32（32位的浮点数，非常好，即保留了一定精度，32位大大降低计算量）\n\n我们可以在这个类中进行数据预处理、数据增强等操作，\n以满足模型的需求。\n\n### 数据迭代器Dataloader","tags":["机器学习"],"categories":["pytorch"]},{"title":"VGG—demo","url":"/2023/08/11/VGG—demo/","content":"# VGG\n参加的一个比赛。\n给出的数据，位移与载荷对应着一个坐标图的x,y的坐标序列，还有两个其他的数据，两个数字而已\n\n起码提交一个模型\n一开始以为RNN。认为是一个时间序列。\n但是评分不高，测试一下CNN。\n打算，根据两个序列，绘制一个对应的二维矩阵，对应点，若有数据，就设置为一。\n使用0/1矩阵，表示一个折线图\n但是数据的差异很大，有一部分（很少一部分，又不一定是异常数据），\n位移最大500/载荷最大617\n大部分数据的载荷集中出现再20-50之间，非常大一部分。\n500*500的矩阵，十分不适合，十分没有效果\n所以打算不处理数据，直接使用。\n\n采用VGG网络\n\n模型运行成功以后，还是效果很差\n学习率0.01，数据集运行10次，标准的VGG网络模型\n导致的结果应该是过拟合了。\n实际效果，数据集，训练了第一次后，该模型给出的输出同一变成同一个值，不变成其他值\n训练准确率，与测试机的准确率只有0.16。主要还是因为这个值对应的数据量稍微多一点。所以再.16/0.17\n不然正常应该十分之一的概率\n\n过拟合原因：\n    1,VGG模型架构太深了\n        VGG减少输入的大小，转化增加通道数。到最好的的通道数达到了512个通道。\n        通道数过多了，或者卷积层深度太深了\n    2，学习率的问题\n        学习率高，导致学习的时候，可能产生学习太快了，对其中一个类别的产生了倾向。\n\n\n后续调整方向\n    学习率：可以再0.001这个级别进行调整\n    数据集训练次数（epoch）：可以调整，增加几次\n    模型：\n        池化层，池化层内核我设置为一，所以没用。\n        内核大小和步幅，填充。都是可以调整","tags":["VGG"],"categories":["神经网络"]},{"title":"Backtracking3","url":"/2023/08/01/Backtracking3/","content":"# 回溯算法\n\n第一个：构建解空间数\n    在这里空间数与剪枝函数相对应\n        剪枝函数---有两种\n            树层裁剪\n                树层裁剪---减少不必要的选项\n                    如果是有序的，寻找边界，可以找到边界的话，这个剪枝条件放在for循环中的条件中\n                    如果是无序，只能放在for循环里面，首先就对当前元素检测，不符合条件的continue；\n            树枝裁剪\n                树枝裁剪---提前结束，不继续往下探索\n                    这种就是条件是递归函数的参数。\n\n\n排列问题（1）：\n集合：[1,2,3,4,5,6,7,8]\n特点：没有重复元素\n\n寻找n个元素的全排列\n\n树枝限定：一个树枝上的元素，同一元素只能用一次，所以要记录用过的元素\n层的限定：应该没有层的限制。横向的限制应该没有\n\n排列问题（2）：\n集合：[1,1,2,3,4,4,5,6,7,8]\n[1,2,1,2]\n特点：存在重复元素s\n\n寻找n个元素的全排列\n\n树枝限定：一个树枝上的元素，同一元素只能用一次，所有要记录用过的元素\n层的限定：在同一层上，相同的元素，不要再用了。限制条件可以使用：先排序，排序后，如果该元素与前一个或者后一个元素相同，则不选择\n\n\n组合问题：\n\n\n","tags":["Backtracking algorithm"],"categories":["算法"]},{"title":"Paper_1","url":"/2023/07/31/Paper-1/","content":"# 论文---边缘缓存\n# Edge Cache\n\n该怎么说呢\n\n现状就是，师姐的第一个工作很厉害，第二个工作交给我。继承下来了，而且第二个工作，虽然没有完成。\n但是已经收到了老师和专家们的肯定了。但是我真看不来苗头。\n\n如何的自圆自话。\n现在的已经建设好的是：\n    第一个功能点吧，缓存分配。\n        这里面两步：\n            第一步：存储空间分配。给AP分配合理大小的缓存空间\n            第二部：存储分区。对分配得到的缓存空间进一步划分，分区，不同的CP内容提供者提供，不同的分区。\n\n    第二个功能点，你在哪里，我好想你呀。爱你的杰\n\n\n根据师姐，老师的话。\n师姐第二个工作是依托于第一个工作的。\n今天看了一下师姐第二个工作\n第一个功能点：预测AP存储空间的分配，时间序列，季节回归模型进行预测的吧\n第二个功能点：流量预测，与内容放置。使用的是HIN（异构信息网络）。内容，AP，等构成序列对，预测什么内容更受欢迎，并将其放置在适合的AP上面。\n\n还有，内容主动请求---主动缓存。\n\n\n那这个工作依托于第一个工作，是第一个工作的其他方面的加强版本。增强版本的话。\n这个功能是否依旧呢。不改功能点，只改方法，换种方法或者优化方法\n\n现在已经优化的有，用户隐私安全，采用的联邦学习。这个第一个功能点加强的部分。\n那么存储空间的分配，本地模型是否依旧采用时间序列，季节回归模型预测呢？\n第一个功能点中的，二次存储分配---存储分区。采用了机器学习（名义上，数据驱动策略，就是一种强化学习，还不是深度强化学习）\n且已经证明多CP的内容分区，具有良好的性能，具有相当好的存储空间利用率\n\n第二个功能点，如何做呢，才能更好。师姐的异构信息网络的提升太大了吧。怎么说，采用偷偷手法，在师姐原有的基础上改进。\n还是换一种新的。但是她对比方案，机器学习（强化学习，不知道是不是，性能与之相比，确实是小巫见大巫了）\n\n或者大胆一点。不做这个流量预测与内容放置了。或者侧重点放在一个小点子上。可以不？\n\n但是边缘缓存方向，不做这个流量预测与内容放置。还有什么呢。重点是，一个功能点已经确定了，还要与第一个功能点相关联","tags":["me"],"categories":["Paper"]},{"title":"Backtracking2","url":"/2023/07/31/Backtracking2/","content":"# 回溯算法（Backtracking algorithm）\n\n回溯算法就是暴力破解吧\n\n不不不！\n\n应该是暴力破解升级版，plus版本\n\n暴力破解：生成所有的可能的解，找符合条件（每个解都生成，都检查一遍）\n\n回溯算法：寻找所有可能的解，符合条件留下。\n\n但：寻找解的过程中，已经不满足条件了，就不继续了，不完整的生成这个不符合条件的解；\n\n\n差异：暴力破解，大多使用多个循环嵌套，寻找可能的解。简单粗暴。但是有时循环的数量不确定。（寻找N元组，eg：N=3,N=4）。而回溯算法采用递归，不用确定循环次数。\n\n使用递归的方式暴力破解，暴力寻找。回溯算法，有限界函数，剪枝函数（限界条件。剪枝条件）。可以更早的结束不必要的寻找。\n\n递归方式的暴力破解，加上剪枝函数，限界函数==回溯算法\n\n所以回溯算法与暴力破解一样，不适用也不能在大数据的情况下使用，内存占用太大\n\n不用递归行不行？毕竟递归函数可以通过栈，队列等数据结构的辅助，转换成非递归的形式。\n\n回溯算法：常常适用于组合排列等问题。要寻找到所有可能的解。\n\n对于组合排列问题：\n    解里面不能有重复元素：{1，1，2，3，8，5，4，6，7，8，4}\n        值不重复，但是元素可以重复：找出所有的三元组的组合，但是每一组合中不能有重复的解\n        值可以重复，元素不可以重复：找出所有的三元组的排列，\n\n    处理方向就两个：\n            横向（宽度）：解空间同一树层，进行筛选\n            纵向（深度）：解空间，同一树枝，进行筛选\n        二者并不冲突，可以分开使用或混合使用","tags":["Backtracking algorithm"],"categories":["算法"]},{"title":"Backtracking","url":"/2023/07/28/Backtracking/","content":"\n# 回溯算法（Backtracking algorithm）\n\n通过迭代的方式在解空间中寻找符合条件的解\n寻找符合条件的组合，排列\n\n解空间，要清楚的构造自己的解空间\n树的形状，去搜索解\n\n终止条件与剪枝函数\n剪枝函数：在递归寻找符合条件的解的过程中，遇到中间过程就不符合条件的，就终止当前，不在继续往下寻找，直接返回\n终止条件：寻找的解，形式上符合结果的形式了，是不是最终的解，还需要多加一重判断。终止条件就是解形式符合结果就行。\n\n剪枝函数放置在哪里呢？\n放在for循环里面更好还是在终止条件里面更好？\n\n放在循环里面可以更快的终止不符合条件的解\n但是放在终止条件，有清晰明了的结构\n\n所以自己判断。在处理节点这里处理\n\n```\nvector<vector<int>> res;\nvector<int> tem;\n\nvoid backtracking(参数) {\n    if (终止条件) {\n        存放结果;\n        return;\n    }\n\n    for (选择：本层集合中元素（树中节点孩子的数量就是集合的大小）) {\n        处理节点;\n        backtracking(路径，选择列表); // 递归\n        回溯，撤销处理结果\n    }\n}\n```\n\n","tags":["Backtracking algorithm"],"categories":["算法"]},{"title":"hexo blog github","url":"/2023/07/27/1/","content":"# hexo--github\n\n# hexo\nHexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他标记语言）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。\n\n既然是框架，就依照框架的流程去使用它。\nhexo是一个工具应用（Application）\n\n使用流程：\n```\n# 第一步安装这个应用程序，hexo-cli是hexo脚手架，就是一个封装，打包好的，更方便用户使用的应用程序形式\nnpm install hexo-cli -g \n# 开始初始化一个博客。其实这一步，就是使用hexo-cli创建一个容器（blog目录，及其里面相关文件），然后下载一个默认主题的博客，放在容器里面。\nhexo init blog\n# 进入刚才使用初始化命令创建的blog目录\ncd blog\n# 下载相关依赖的库文件，包文件，第三方模块\nnpm install\n# 启动hexo web程序，博客网站可以正常运行起来，此时可以浏览器输入localhost：4000，可以看到默认的hexo博客网站\nhexo server\n```\n\n## 主题设置\n单纯方便，还不够吸引人，吸引人还有其相应的主题\n\n博客不同的样式就是其不同的主题\n\n应用程序生成的blog目录下，有个themes文件夹，可以下载其他的主题放置在这个文件夹里面。\n![](./../img/image.png)\n我这里有三个主题，我所下载的，fuild，resume-docs，Sw-blog这三个。默认主题landscape不在里面。\n![](./../img/image-1.png)\n而每个主题里面的文件目录结构应该一致，大差不差\n![](./../img/image-2.png)\n\n\n其次是配置主题，哪怕是同一个主题，设置不同，也会有不同的差异\n每个主题里面都有一个_config.yml。可以在里面查看可以自己设置的设置\n\nblog目录下，也有一个_config.yml。这个整个本地hexo的设置。两者相互配合。\n若有存在对一个属性不同的设置，主题里面的生效，优先级更高。\n\n# 配置GitHub\n\n## 创建一个仓库repositories\n\n仓库的名字：[GitHub_name].github.io\nAdd a README file这个选项，可选可不选。\n勾上：就是创建readme文件，默认创建仓库main分支了。\n不勾上：就是一个空的仓库，没有分支，里面啥都没有。\n![](./../img/image-3.png)\n在创建的这个仓库里面的settings里面设置\n左侧General设置：Default branch。默认是main，但是我本地使用git上传文件，创建使用的master分支，所以我修改了\n![](./../img/image-4.png)\n左侧Pagers设置：Branch选择blog网站文件那个分支，第二个/root，是要求在该目录下能找到index.html文件。\n这个设置，就是告诉github，在我这个仓库的该分支的该目录下，找到index.html文件，并在人家访问[GitHub_name].github.io网址的时候，响应这个网页。\n![](./../img/image-5.png)\n\n\n## 上传blog网站文件\nblog网站文件怎么获取，使用hexo这个应用程序\n\n```\n# 清楚缓存，清楚之前产生的文件。\nhexo clean\n# 这个命令会产生 .deploy_git文件夹。这个文件和github主仓库的文件一样，这个就是使用hexo，它按照要求（主题，配置文件，自己写的markdown文件），自动生成的blog网站文件\nhexo generate \n# 自动deploy（部署），在_config.yml配置\nhexo deploy\n```\n![](./../img/image.png)\n![](./../img/image-6.png)\n\n\n\n# 提示\n\n配置就这些，对应上了就行\n官方文件要一直看，跟着官方文件走 [hexo官网](https://hexo.io/zh-cn/ \"Title\")","tags":["hexo"],"categories":["hexo"]},{"title":"Hello World","url":"/2023/07/27/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo clean\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","tags":["Hello World"],"categories":["Hello World"]}]